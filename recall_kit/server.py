"""
OpenAI-compatible API server for Recall Kit.

This module provides a FastAPI server that implements an OpenAI-compatible API
for chat completions with memory augmentation.
"""

from __future__ import annotations

import logging
from typing import Any, Dict, List, Optional

from fastapi import FastAPI, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

from recall_kit import RecallKit
from .cli import consolidate

# Set up logging
logger = logging.getLogger(__name__)


class Message(BaseModel):
    """A chat message."""
    role: str = Field(..., description="The role of the message sender (system, user, assistant)")
    content: str = Field(..., description="The content of the message")


class ChatCompletionRequest(BaseModel):
    """Request for a chat completion."""
    model: str = Field(..., description="The model to use for completion")
    messages: List[Message] = Field(..., description="The messages to generate a completion for")
    temperature: Optional[float] = Field(0.7, description="Sampling temperature")
    max_tokens: Optional[int] = Field(None, description="Maximum number of tokens to generate")
    stream: Optional[bool] = Field(False, description="Whether to stream the response")


class ChatCompletionChoice(BaseModel):
    """A chat completion choice."""
    index: int = Field(..., description="The index of the choice")
    message: Message = Field(..., description="The message generated by the model")
    finish_reason: str = Field("stop", description="The reason the model stopped generating")


class ChatCompletionUsage(BaseModel):
    """Usage information for a chat completion."""
    prompt_tokens: int = Field(..., description="Number of tokens in the prompt")
    completion_tokens: int = Field(..., description="Number of tokens in the completion")
    total_tokens: int = Field(..., description="Total number of tokens used")


class MessageResponse(BaseModel):
    """Response for a message."""
    id: str = Field(..., description="The ID of the message")
    role: str = Field(..., description="The role of the message sender")
    content: str = Field(..., description="The content of the message")
    created_at: str = Field(..., description="The timestamp when the message was created")
    metadata: dict = Field({}, description="Additional metadata about the message")


class MessageSetResponse(BaseModel):
    """Response for a message set."""
    id: str = Field(..., description="The ID of the message set")
    message_ids: List[str] = Field(..., description="The IDs of messages in this set")
    active: bool = Field(..., description="Whether this message set is active")
    created_at: str = Field(..., description="The timestamp when the message set was created")
    metadata: dict = Field({}, description="Additional metadata about the message set")


class ChatCompletionResponse(BaseModel):
    """Response from a chat completion."""
    id: str = Field(..., description="The ID of the completion")
    object: str = Field("chat.completion", description="The object type")
    created: int = Field(..., description="The Unix timestamp of when the completion was created")
    model: str = Field(..., description="The model used for completion")
    choices: List[ChatCompletionChoice] = Field(..., description="The completion choices")
    usage: ChatCompletionUsage = Field(..., description="Usage information")


def get_recall_kit() -> RecallKit:
    """Get the RecallKit instance."""
    return RecallKit()


def create_app(
    recall: RecallKit,
) -> FastAPI:
    """
    Create a FastAPI app for the Recall Kit server.

    Args:
        memory_db_path: Path to the memory database
        auto_consolidate: Whether to automatically consolidate memories
        model: The model to use for completions and consolidation

    Returns:
        A FastAPI app
    """
    app = FastAPI(
        title="Recall Kit API",
        description="OpenAI-compatible API for chat completions with memory augmentation",
        version="0.1.0",
    )

    # Add CORS middleware
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    # Override the get_recall_kit dependency
    app.dependency_overrides[get_recall_kit] = lambda: recall

    # Import litellm here to avoid requiring it for non-server use
    try:
        import litellm
    except ImportError:
        raise ImportError(
            "litellm is required for the server. "
            "Install it with: pip install litellm"
        )

    @app.post("/v1/chat/completions", response_model=ChatCompletionResponse)
    async def create_chat_completion(
        request: ChatCompletionRequest,
        recall_kit: RecallKit = Depends(get_recall_kit),
    ) -> ChatCompletionResponse:
        """
        Create a chat completion with memory augmentation.

        Args:
            request: The chat completion request
            recall_kit: The RecallKit instance

        Returns:
            The chat completion response
        """
        try:
            # Extract the user's query from the last user message
            user_messages = [m for m in request.messages if m.role == "user"]
            if not user_messages:
                raise HTTPException(status_code=400, detail="No user messages provided")

            # Convert the request to a dictionary for processing
            request_dict = {
                "model": request.model,
                "messages": [{"role": m.role, "content": m.content} for m in request.messages],
                "temperature": request.temperature,
            }

            if request.max_tokens:
                request_dict["max_tokens"] = request.max_tokens

            # No need to set api_key or api_base, they are handled by the completion function

            # Use the chat_completion method which handles the entire pipeline
            response = recall_kit.completion(**request_dict)

            # Always perform auto-consolidation after each chat completion
            # Use LLM-driven consolidation
            recall_kit.consolidate_memories(model=request.model)

            # Convert the litellm response to our response format
            return ChatCompletionResponse(
                id=response.id,
                created=int(response.created),
                model=response.model,
                choices=[
                    ChatCompletionChoice(
                        index=i,
                        message=Message(
                            role=choice.message.role,
                            content=choice.message.content,
                        ),
                        finish_reason=choice.finish_reason,
                    )
                    for i, choice in enumerate(response.choices)
                ],
                usage=ChatCompletionUsage(
                    prompt_tokens=response.usage.prompt_tokens,
                    completion_tokens=response.usage.completion_tokens,
                    total_tokens=response.usage.total_tokens,
                ),
            )

        except Exception as e:
            logger.exception("Error creating chat completion")
            raise HTTPException(status_code=500, detail=str(e))

    @app.get("/v1/messages", response_model=List[MessageResponse])
    async def get_messages(
        recall_kit: RecallKit = Depends(get_recall_kit),
    ) -> List[MessageResponse]:
        """
        Get all messages.

        Args:
            recall_kit: The RecallKit instance

        Returns:
            List of all messages
        """
        try:
            messages = recall_kit.get_all_messages()
            return [
                MessageResponse(
                    id=message.id,
                    role=message.role,
                    content=message.content,
                    created_at=message.created_at.isoformat(),
                    metadata=message.metadata,
                )
                for message in messages
            ]
        except Exception as e:
            logger.exception("Error getting messages")
            raise HTTPException(status_code=500, detail=str(e))

    @app.get("/v1/messages/{message_id}", response_model=MessageResponse)
    async def get_message(
        message_id: str,
        recall_kit: RecallKit = Depends(get_recall_kit),
    ) -> MessageResponse:
        """
        Get a message by ID.

        Args:
            message_id: The ID of the message to retrieve
            recall_kit: The RecallKit instance

        Returns:
            The message
        """
        try:
            message = recall_kit.get_message(message_id)
            if not message:
                raise HTTPException(status_code=404, detail=f"Message {message_id} not found")

            return MessageResponse(
                id=message.id,
                role=message.role,
                content=message.content,
                created_at=message.created_at.isoformat(),
                metadata=message.metadata,
            )
        except HTTPException:
            raise
        except Exception as e:
            logger.exception(f"Error getting message {message_id}")
            raise HTTPException(status_code=500, detail=str(e))

    @app.get("/v1/message-sets", response_model=List[MessageSetResponse])
    async def get_message_sets(
        recall_kit: RecallKit = Depends(get_recall_kit),
    ) -> List[MessageSetResponse]:
        """
        Get all message sets.

        Args:
            recall_kit: The RecallKit instance

        Returns:
            List of all message sets
        """
        try:
            # This is not implemented in the RecallKit class, so we'll need to get it from the storage
            message_sets = recall_kit.storage.get_all_message_sets()
            return [
                MessageSetResponse(
                    id=message_set.id,
                    message_ids=message_set.message_ids,
                    active=message_set.active,
                    created_at=message_set.created_at.isoformat(),
                    metadata=message_set.metadata,
                )
                for message_set in message_sets
            ]
        except Exception as e:
            logger.exception("Error getting message sets")
            raise HTTPException(status_code=500, detail=str(e))

    @app.get("/v1/message-sets/active", response_model=MessageSetResponse)
    async def get_active_message_set(
        recall_kit: RecallKit = Depends(get_recall_kit),
    ) -> MessageSetResponse:
        """
        Get the active message set.

        Args:
            recall_kit: The RecallKit instance

        Returns:
            The active message set
        """
        try:
            message_set = recall_kit.get_active_message_set()
            if not message_set:
                raise HTTPException(status_code=404, detail="No active message set found")

            return MessageSetResponse(
                id=message_set.id,
                message_ids=message_set.message_ids,
                active=message_set.active,
                created_at=message_set.created_at.isoformat(),
                metadata=message_set.metadata,
            )
        except HTTPException:
            raise
        except Exception as e:
            logger.exception("Error getting active message set")
            raise HTTPException(status_code=500, detail=str(e))

    @app.get("/v1/message-sets/{message_set_id}", response_model=MessageSetResponse)
    async def get_message_set(
        message_set_id: str,
        recall_kit: RecallKit = Depends(get_recall_kit),
    ) -> MessageSetResponse:
        """
        Get a message set by ID.

        Args:
            message_set_id: The ID of the message set to retrieve
            recall_kit: The RecallKit instance

        Returns:
            The message set
        """
        try:
            message_set = recall_kit.get_message_set(message_set_id)
            if not message_set:
                raise HTTPException(status_code=404, detail=f"Message set {message_set_id} not found")

            return MessageSetResponse(
                id=message_set.id,
                message_ids=message_set.message_ids,
                active=message_set.active,
                created_at=message_set.created_at.isoformat(),
                metadata=message_set.metadata,
            )
        except HTTPException:
            raise
        except Exception as e:
            logger.exception(f"Error getting message set {message_set_id}")
            raise HTTPException(status_code=500, detail=str(e))

    @app.get("/v1/message-sets/{message_set_id}/messages", response_model=List[MessageResponse])
    async def get_messages_in_set(
        message_set_id: str,
        recall_kit: RecallKit = Depends(get_recall_kit),
    ) -> List[MessageResponse]:
        """
        Get all messages in a message set.

        Args:
            message_set_id: The ID of the message set
            recall_kit: The RecallKit instance

        Returns:
            List of messages in the message set
        """
        try:
            message_set = recall_kit.get_message_set(message_set_id)
            if not message_set:
                raise HTTPException(status_code=404, detail=f"Message set {message_set_id} not found")

            messages = recall_kit.get_messages_in_set(message_set_id)
            return [
                MessageResponse(
                    id=message.id,
                    role=message.role,
                    content=message.content,
                    created_at=message.created_at.isoformat(),
                    metadata=message.metadata,
                )
                for message in messages
            ]
        except HTTPException:
            raise
        except Exception as e:
            logger.exception(f"Error getting messages in set {message_set_id}")
            raise HTTPException(status_code=500, detail=str(e))

    @app.get("/v1/models")
    async def list_models() -> Dict[str, Any]:
        """
        List available models.

        Returns:
            A list of available models
        """
        try:
            # Return a list of common models since litellm.model_list is not callable
            return {
                "object": "list",
                "data": [
                    {"id": "gpt-4", "object": "model"},
                    {"id": "gpt-4-turbo", "object": "model"},
                    {"id": "gpt-3.5-turbo", "object": "model"},
                    {"id": "text-embedding-3-small", "object": "model"},
                    {"id": "text-embedding-3-large", "object": "model"},
                ],
            }
        except Exception as e:
            logger.exception("Error listing models")
            # Return a minimal response with common models
            return {
                "object": "list",
                "data": [
                    {"id": "gpt-4", "object": "model"},
                    {"id": "gpt-4-turbo", "object": "model"},
                    {"id": "gpt-3.5-turbo", "object": "model"},
                ],
            }

